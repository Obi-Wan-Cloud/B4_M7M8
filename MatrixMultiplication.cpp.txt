%%writefile distributed_matrix_mult.cpp
    #include <iostream>
    #include <cstdlib>
    #include <ctime>
    #include <mpi.h>

    #define N 2000 // Matrix dimensions (NxN)

    // Initialize matrices A, B, and C (only on rank 0)
    void init(double*& A, double*& B, double*& C) {
        A = new double[N * N];
        B = new double[N * N];
        C = new double[N * N];

        for (unsigned i = 0; i < N * N; i++) {
            A[i] = double(i) / N;
            B[i] = double(i) / N;
            C[i] = 0.0;
        }
    }

    int main(int argc, char** argv) {
        MPI_Init(&argc, &argv);

        int rank, size;
        MPI_Comm_rank(MPI_COMM_WORLD, &rank);
        MPI_Comm_size(MPI_COMM_WORLD, &size);

        std::cerr << "Process " << rank << " of " << size << " started." << std::endl; // Added: Indicate process start

        double *A = nullptr, *B = nullptr, *C = nullptr;
        double *local_A = nullptr, *local_C = nullptr;

        int rows_per_process = N / size;
        int leftover_rows = N % size;

        double start_time, end_time;

        if (rank == 0) {
            // Initialize matrices A, B, and C
            init(A, B, C);
            start_time = MPI_Wtime(); // Start timer on rank 0
            std::cerr << "Rank 0 initialized matrices and started timer." << std::endl; // Added: Indicate rank 0 initialization
        }

        // Allocate memory for local matrices
        int local_rows = rows_per_process + (rank < leftover_rows ? 1 : 0);
        local_A = new double[local_rows * N];
        local_C = new double[local_rows * N]();
        std::cerr << "Process " << rank << " allocated local memory." << std::endl; // Added: Indicate local memory allocation

        // Scatter rows of A among processes
        int* sendcounts = nullptr;
        int* displs = nullptr;

        if (rank == 0) {
            sendcounts = new int[size];
            displs = new int[size];

            int offset = 0;
            for (int i = 0; i < size; i++) {
                int rows = rows_per_process + (i < leftover_rows ? 1 : 0);
                sendcounts[i] = rows * N;
                displs[i] = offset;
                offset += rows * N;
            }
            std::cerr << "Rank 0 prepared scatter parameters." << std::endl; // Added: Indicate scatter parameters preparation
        }

        MPI_Scatterv(A, sendcounts, displs, MPI_DOUBLE, local_A, local_rows * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);
        std::cerr << "Process " << rank << " finished scattering A." << std::endl; // Added: Indicate scatter completion

        // Broadcast matrix B to all processes
        if (rank != 0) { // Only allocate on non-root ranks, as root already has B from init
            B = new double[N * N];
        }
        MPI_Bcast(B, N * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);
        std::cerr << "Process " << rank << " finished broadcasting B." << std::endl; // Added: Indicate broadcast completion

        // Perform local computation of C = A * B
        std::cerr << "Process " << rank << " starting local computation." << std::endl; // Added: Indicate start of local computation
        for (int i = 0; i < local_rows; i++) {
            for (int j = 0; j < N; j++) {
                for (int k = 0; k < N; k++) {
                    local_C[i * N + j] += local_A[i * N + k] * B[k * N + j];
                }
            }
        }
        std::cerr << "Process " << rank << " finished local computation." << std::endl; // Added: Indicate local computation completion

        // Gather results back to rank 0
        MPI_Gatherv(local_C, local_rows * N, MPI_DOUBLE, C, sendcounts, displs, MPI_DOUBLE, 0, MPI_COMM_WORLD);
        std::cerr << "Process " << rank << " finished gathering results." << std::endl; // Added: Indicate gather completion

        if (rank == 0) {
            end_time = MPI_Wtime(); // End timer on rank 0
            std::cout << "Execution time: " << end_time - start_time << " seconds" << std::endl; // Original output
            std::cerr << "Rank 0 printed execution time." << std::endl; // Added: Indicate timing print
        }

        // Cleanup
        delete[] local_A;
        delete[] local_C;
        if (rank == 0) {
            delete[] sendcounts;
            delete[] displs;
            delete[] A;
            delete[] B; // Rank 0 allocated B in init
            delete[] C;
            std::cerr << "Rank 0 cleaned up memory." << std::endl; // Added: Indicate rank 0 cleanup
        } else {
            delete[] B; // Non-rank 0 allocated B after broadcast
            std::cerr << "Process " << rank << " cleaned up local memory." << std::endl; // Added: Indicate non-rank 0 cleanup
        }

        MPI_Finalize();
        std::cerr << "Process " << rank << " finished MPI." << std::endl; // Added: Indicate MPI finalization
        return 0;
    }